---
id: "CVAM-20"
title: "LLM-HYPE: Generative CTR Modeling for Cold-Start Ad Personalization via Multimodal LLM-Based Hypernetworks"
authors:
- Luyi Ma
- Wanjia Zhang
- Shubham Thakur
- Kehui Yao
- Kai Zhao
- Ayush Agarwal
- Zezhong Fan
- Rahul Iyer
- Jason Cho
- Jianpeng Xu
- Evren Korpeoglu
- Sushant Kumar
- Kannan Achan
type: "short"
arxiv_link:
abstract: "In online advertising platforms, new promotional ads suffer from the cold-start problem when newly introduced ads lack sufficient user feedback for model training. In this work, we propose LLM-HYPE, a novel framework that treats large language models (LLMs) as hypernetworks to directly generate the parameters of the click-through rate (CTR) estimator—without any training labels. LLM-HYPE uses few-shot Chain-of-Thought prompting over multi-modal ad content (text and images) to infer feature-wise model weights for a linear CTR predictor. By retrieving semantically similar past campaigns via CLIP embeddings and formatting them into prompt-based demonstrations, the LLM learns to reason about customer intent, feature influence, and content relevance. To ensure numerical stability and serve-ability, we introduce normalization and calibration techniques that align the generated weights with production-ready CTR distributions. Extensive offline experiments show that LLM-HYPE significantly outperforms cold-start baselines and even surpasses traditionally trained models in NDCG@10. A 30-day online A/B testing demonstrates that the generated models achieve CTR performance comparable to continuously trained systems—without incurring labeling or retraining costs."
---

---
id: "CVAM-15"
title: "Align Before You Recommend: Parameter Efficient Personalization via Cross-Attentive Fusion of Hierarchical Language Models"
authors:
- Alicja Kwasniewska
- Chad Neal
- Marcin Bednarz
type: "full"
arxiv_link:
abstract: "The rapidly growing global advertising and marketing industry demands innovative machine learning systems that balance accuracy with efficiency. Recommendation systems, crucial to many platforms, require careful considerations and potential enhancements. While Large Language Models (LLMs) have transformed various domains, their potential in sequential recommendation systems remains under-explored. Pioneering works like Hierarchical Large Language Models (HLLM) demonstrated LLMsâ€™ capability for next-item recommendation but rely on computationally intensive fine-tuning, limiting widespread adoption. This work introduces HLLM+, enhancing the HLLM framework to achieve high-accuracy recommendations without full model fine-tuning. By introducing targeted alignment components between frozen LLMs, our approach matches fully-tuned model performance in popular item recommendation tasks (recall/NCDG @5/@10) while reducing training time by 30.7%. We also propose a ranking-aware loss adjustment, improving convergence and recommendation quality for popular items. Experiments show HLLM+ achieves superior performance with frozen item representations, improving recall@5 by up to 52% compared to baseline frozen models. These findings are significant for the advertising technology sector, where rapid adaptation and efficient deployment across brands are essential for maintaining competitive advantage."
---
